import os
import time
import pickle
import json
import argparse
from datetime import datetime

from google import genai
from pytz import timezone
from tqdm import tqdm


EVAL_think_general_guide_prompt = """I will provide you with a dialog and a response generated by a CRS.

Dialog:
%s

Response:
%s

Evaluate the response from each competitor in terms of informativeness, fluency, and relevance on a scale 0 to 5 (0: very bad, 5: very good).

1) Informativeness: Whether the response incorporates rich knowledge
2) Fluency: Whether the response includes diverse words
3) Relevance: Whether the response provides explanations about the recommended item with its features relevant to the dialog context

Grading rules:
- Be conservative when assigning scores. Do not give 4 or 5 unless the response clearly excels. 
- If a criterion is only partially satisfied, reduce the score (e.g., give 1 or 2 instead of 3). 
- Only responses with strong, explicit evidence of meeting the criterion should receive scores above 3. 
- Use 0 freely for responses that fail to meet the requirement at all.

The reasoning process and answer are enclosed within <think></think> and <answer></answer> tags, respectively, i.e., <think> reasoning process here</think> <answer>answer here</answer>

### Output:
<think>{think}</think>
<answer>{"informativeness": <score 0-5>, "fluency": <score 0-5>, "relevance": <score 0-5>}</answer>
"""


EVAL_scoring_guide_prompt = """I will provide you with a dialog and a response generated by a CRS.

Dialog:
%s

Response:
%s

Evaluate the response from each competitor in terms of informativeness, fluency, and relevance on a scale 1 to 5 (1: very bad, 5: very good).

1) Informativeness: Whether the response incorporates rich knowledge
2) Fluency: Whether the response includes diverse words
3) Relevance: Whether the response recommends proper items and their explanatory features relevant to the dialog context

Guideline for scoring:
- 1 point: The response is very poor. It fails almost entirely to meet this criterion.
- 2 points: The response partially meets this criterion. It shows some adequacy but remains weak or insufficient.
- 3 points: The response meets the minimum requirement for this criterion. It is acceptable but lacks depth, richness, or distinct strengths.
- 4 points: The response performs well on this criterion. It is clear, specific, natural, and contextually appropriate, but not outstanding.
- 5 points: The response fully excels in this criterion. It is rich, highly natural, and strongly aligned with the context. Award only when it clearly stands out.

The reasoning process and answer are enclosed within <think></think> and <answer></answer> tags, respectively, i.e., <think> reasoning process here</think> <answer>answer here</answer>

### Output:
<think>reasoning precess here</think>
<answer>{"informativeness": <score 1-5>, "fluency": <score 1-5>, "relevance": <score 1-5>}</answer>"""


EVAL_scoring_guide_for_target_item_prompt = """I will provide you with a dialog and a response generated by a CRS.

Dialog:
%s

Response:
%s

Evaluate the response in terms of informativeness, fluency, and relevance on a scale from 1 to 5 (1: very bad, 5: very good), under the assumption that the CRS should recommend the target item '%s' in its response.

1) Informativeness: Whether the response incorporates rich knowledge
2) Fluency: Whether the response includes diverse words
3) Relevance: Whether the response recommends the target item appropriately and provides explanatory features relevant to the dialog context

Guideline for scoring:
1 point: The response is very poor. It fails almost entirely to meet this criterion.  
2 points: The response partially meets this criterion. It shows some adequacy but remains weak or insufficient.  
3 points: The response meets the minimum requirement for this criterion. It is acceptable but lacks depth, richness, or distinct strengths.  
4 points: The response performs well on this criterion. It is clear, specific, natural, and contextually appropriate, but not outstanding.  
5 points: The response fully excels in this criterion. It is rich, highly natural, and strongly aligned with the target item and the dialog context. Award only when it clearly stands out.

The reasoning process and answer are enclosed within <think></think> and <answer></answer> tags, respectively, i.e., <think>reasoning process here</think> <answer>answer here</answer>

### Output:
<think>reasoning process here</think>
<answer>{"informativeness": <score 1-5>, "fluency": <score 1-5>, "relevance": <score 1-5>}</answer>"""


EVAL_two_dim = """I will provide you with a dialog and a response generated by a Conversational Recommender System (CRS).

Dialog:
%s

Response:
%s

Evaluate the response along two dimensions: (A) Recommendation Quality and (B) Explanation Quality.

A. Recommendation Quality
1) Validity: Does the recommended item align with the user’s preferences expressed in the dialog?

B. Explanation Quality
1) Informativeness: Does the explanation incorporate rich and meaningful knowledge about the item?
2) Fluency: Is the explanation natural, coherent, and expressed with varied wording?
3) Relevance: Does the explanation highlight item features that are directly relevant to the dialog context?

Scoring: Use a 1–5 scale for each criterion.
- 1 point: Very poor. Fails almost entirely to meet the criterion.
- 2 points: Weak. Shows partial adequacy but remains insufficient.
- 3 points: Moderate. Meets the minimum requirement but lacks depth or strength.
- 4 points: Good. Clear, specific, and contextually appropriate, though not outstanding.
- 5 points: Excellent. Rich, highly natural, and strongly aligned with the context. Award only if it clearly stands out.

Output format:
<think>reasoning process here</think>
<answer>{"validity": <1–5>, "informativeness": <1–5>, "fluency": <1–5>, "relevance": <1–5>}</answer>"""


EVAL_only_response = """I will provide you with a dialog and a response generated by a Conversational Recommender System (CRS).

Dialog:
%s

Response:
%s

Evaluate the response along explanation quality.
1) Informativeness: Does the explanation incorporate rich and meaningful knowledge about the recommended item?
2) Fluency: Is the explanation natural, coherent, and expressed with varied wording?
3) Relevance: Does the explanation highlight the features of the recommended item that are directly relevant to the dialog context?

Scoring: Use a 1–5 scale for each criterion.
- 1 point: Very poor. Fails almost entirely to meet the criterion.
- 2 points: Weak. Shows partial adequacy but remains insufficient.
- 3 points: Moderate. Meets the minimum requirement but lacks depth or strength.
- 4 points: Good. Clear, specific, and contextually appropriate, though not outstanding.
- 5 points: Excellent. Rich, highly natural, and strongly aligned with the context. Award only if it clearly stands out.

Output format:
<think>reasoning process here</think>
<answer>{"informativeness": <1–5>, "fluency": <1–5>, "relevance": <1–5>}</answer>"""


Normal_Evaluator = """I will provide you with a dialog and a response generated by a Conversational Recommender System (CRS).

Dialog:
%s

Response:
%s

Evaluate the response along explanation quality.
1) Informativeness: Does the explanation incorporate rich and meaningful knowledge about the recommended item?
2) Fluency: Is the explanation natural, coherent, and expressed with varied wording?
3) Relevance: Does the explanation highlight the features of the recommended item that are directly relevant to the dialog context?

Scoring: Use a 1–5 scale for each criterion.
- 1 point: Very poor. Fails almost entirely to meet the criterion.
- 2 points: Weak. Shows partial adequacy but remains insufficient.
- 3 points: Moderate. Meets the minimum requirement but lacks depth or strength.
- 4 points: Good. Clear, specific, and contextually appropriate, though not outstanding.
- 5 points: Excellent. Rich, highly natural, and strongly aligned with the context. Award only if it clearly stands out.

Output format:
<think>reasoning process here</think>
<answer>{"informativeness": <1–5>, "fluency": <1–5>, "relevance": <1–5>}</answer>"""


def parse_args():
    parser = argparse.ArgumentParser()
    parser.add_argument('--access_token', type=str, default="")
    parser.add_argument('--gemini_model', type=str, default="gemini-2.5-pro")
    parser.add_argument('--cnt', type=int, default=0)
    parser.add_argument('--log_name', type=str, default="")

    parser.add_argument('--dataset_path', type=str, default="")
    parser.add_argument('--response_path', type=str, default="")

    parser.add_argument('--only_6_turn', action='store_true')
    args = parser.parse_args()
    return args


if __name__=='__main__':
    args = parse_args()
    client = genai.Client(api_key=args.access_token)


    if 'pkl' in args.dataset_path:
        dataset = pickle.load(open(args.dataset_path, 'rb'))
    elif 'json' in args.dataset_path:
        dataset = json.load(open(args.dataset_path, 'r', encoding='utf-8'))
    else:
        print('Dataset not found.')

    # dataset = pickle.load(open(args.dataset_path, 'rb'))
    gpt_response = json.load(open(args.response_path, 'r', encoding='utf-8'))

    prompt = Normal_Evaluator

    instructions = []
    for idx, (data, response) in enumerate(zip(dataset, gpt_response)):
        dialog = data['dialog']
        if args.only_6_turn:
            dialogs = dialog.split('\n')[-6:]
            dialog = ''
            for d in dialogs:
                dialog += d
                dialog += '\n'
            dialog = dialog.strip()

        response_key = list(gpt_response[0].keys())[-1]
        response = response[response_key]
        if not response.strip().startswith('System: '):
            response = 'System: ' + response
        # target_item = data['topic']

        instruction = prompt % (dialog, response)
        instructions.append(instruction)

    mdhm = str(datetime.now(timezone('Asia/Seoul')).strftime('%m%d%H%M%S'))
    if args.log_name == '':
        log_name = f'evaluation/gemini_eval/{mdhm}_{args.gemini_model}_result.json'
    else:
        log_name = f'evaluation/gemini_eval/{mdhm}_{args.gemini_model}_eval_{args.log_name}'

    args.log_file = open(os.path.join(log_name), 'a', buffering=1, encoding='UTF-8')


    if 'train' in args.dataset_path:
        print('EVAL Train!')
    elif 'test' in args.dataset_path:
        print('Eval Test!')
    print('RESPONSE FILE: ', args.response_path)
    print('EVAL RESULT FILE: ', args.log_file)

    # Evaluation 시작
    print('GEMINI START')
    cnt = args.cnt
    # for instruction in tqdm(instructions[cnt:], bar_format=' {percentage:3.0f} % | {bar:23} {r_bar}'):
    pbar = tqdm(total=len(instructions), initial=cnt, bar_format=' {percentage:3.0f} % | {bar:30} {n_fmt}/{total_fmt}')
    while cnt < len(instructions):
        instruction = instructions[cnt]
        try:
            response = client.models.generate_content(model=args.gemini_model, contents=instruction)
            response = response.text

            args.log_file.write(
                json.dumps({'INPUT': instruction, 'OUTPUT': response}, ensure_ascii=False, indent=4) + '\n')
            cnt += 1
            pbar.update(1)

        except Exception as error:
            print("%s | ERROR cnt: %d" % (error, cnt))
            print(args.log_file)
            # args.cnt = int(cnt)
            time.sleep(5)
