
import os
import time
import pickle
import json
import argparse
from datetime import datetime

from openai import OpenAI
from pytz import timezone
from tqdm import tqdm


GPT_EVAL_prompt = """I will provide you with a dialog and a response generated by a CRS.

Dialog:
{dialog}

Response:
{response}

Evaluate the response from each competitor in terms of informativeness, fluency, and relevance on a scale 0 to 5 (0: very bad, 5: very good).

1) Informativeness: Whether the response incorporates rich knowledge
2) Fluency: Whether the response includes diverse words
3) Relevance: Whether the response provides explanations about the recommended item with its features relevant to the dialog context

### Output Format
{"informativeness": <score 0-5>, "fluency": <score 0-5>, "relevance": <score 0-5>}"""



GPT_EVAL_think_prompt = """I will provide you with a dialog and a response generated by a CRS.

Dialog:
%s

Response:
%s

Evaluate the response from each competitor in terms of informativeness, fluency, and relevance on a scale 0 to 5 (0: very bad, 5: very good).

1) Informativeness: Whether the response incorporates rich knowledge
2) Fluency: Whether the response includes diverse words
3) Relevance: Whether the response provides explanations about the recommended item with its features relevant to the dialog context

The reasoning process and answer are enclosed within <think></think> and <answer></answer> tags, respectively, i.e., <think> reasoning process here</think> <answer>answer here</answer>

### Output:
<think>{think}</think>
<answer>{"informativeness": <score 0-5>, "fluency": <score 0-5>, "relevance": <score 0-5>}</answer>"""


GPT_EVAL_think_general_guide_prompt = """I will provide you with a dialog and a response generated by a CRS.

Dialog:
%s

Response:
%s

Evaluate the response from each competitor in terms of informativeness, fluency, and relevance on a scale 0 to 5 (0: very bad, 5: very good).

1) Informativeness: Whether the response incorporates rich knowledge
2) Fluency: Whether the response includes diverse words
3) Relevance: Whether the response provides explanations about the recommended item with its features relevant to the dialog context

Grading rules:
- Be conservative when assigning scores. Do not give 4 or 5 unless the response clearly excels. 
- If a criterion is only partially satisfied, reduce the score (e.g., give 1 or 2 instead of 3). 
- Only responses with strong, explicit evidence of meeting the criterion should receive scores above 3. 
- Use 0 freely for responses that fail to meet the requirement at all.

The reasoning process and answer are enclosed within <think></think> and <answer></answer> tags, respectively, i.e., <think> reasoning process here</think> <answer>answer here</answer>

### Output:
<think>{think}</think>
<answer>{"informativeness": <score 0-5>, "fluency": <score 0-5>, "relevance": <score 0-5>}</answer>"""


EVAL_scoring_guide_prompt = """I will provide you with a dialog and a response generated by a CRS.

Dialog:
%s

Response:
%s

Evaluate the response from each competitor in terms of informativeness, fluency, and relevance on a scale 1 to 5 (1: very bad, 5: very good).

1) Informativeness: Whether the response incorporates rich knowledge
2) Fluency: Whether the response includes diverse words
3) Relevance: Whether the response provides explanations about the recommended item with its features relevant to the dialog context

Guideline for scoring:
- 1 point: The response is very poor. It fails almost entirely to meet this criterion.
- 2 points: The response partially meets this criterion. It shows some adequacy but remains weak or insufficient.
- 3 points: The response meets the minimum requirement for this criterion. It is acceptable but lacks depth, richness, or distinct strengths.
- 4 points: The response performs well on this criterion. It is clear, specific, natural, and contextually appropriate, but not outstanding.
- 5 points: The response fully excels in this criterion. It is rich, highly natural, and strongly aligned with the context. Award only when it clearly stands out.

The reasoning process and answer are enclosed within <think></think> and <answer></answer> tags, respectively, i.e., <think> reasoning process here</think> <answer>answer here</answer>

### Output:
<think>reasoning precess here</think>
<answer>{"informativeness": <score 1-5>, "fluency": <score 1-5>, "relevance": <score 1-5>}</answer>"""



def parse_args():
    parser = argparse.ArgumentParser()
    parser.add_argument('--access_token', type=str, default="")
    parser.add_argument('--cnt', type=int, default=0)
    parser.add_argument('--log_name', type=str, default="")
    args = parser.parse_args()
    return args


def execute(args,
            instructions: list = None,
            labels: list = None,
            inputs: list = None):
    # openai.api_key = args.access_token
    client = OpenAI(api_key=args.access_token)

    cnt = args.cnt
    for instruction in tqdm(instructions[cnt:], bar_format=' {percentage:3.0f} % | {bar:23} {r_bar}'):

        # content = template % (instruction)
        # content = mentioned_reviews_concat + refinement_template % instruction
        # print()
        try:
            response = client.chat.completions.create(
                model=MODEL,
                messages=[
                    {"role": "user",
                     # "content": review_summary_template % (label, instruction, label)}
                     "content": instruction}
                ],
                temperature=0,
            )

            response = response.choices[0].message.content

            args.log_file.write(json.dumps({'INPUT': instruction, 'OUTPUT': response}, ensure_ascii=False, indent=4) + '\n')
            cnt += 1

        except Exception as error:
            print("ERROR cnt: %d" % (cnt))
            print(args.log_file)
            args.cnt = int(cnt)
            time.sleep(5)
            break

        # openai.api_requestor._thread_context.session.close()
        if int(cnt) == len(instructions):
            return False



def chatgpt_test(args,
                 instructions: list = None,
                 labels: list = None,
                 inputs: list = None,
                 ):
    print('CHATGPT_TEST_START')
    while True:
        if execute(args=args, instructions=instructions, labels=labels, inputs=inputs) == False:
            break



if __name__ == "__main__":

    args = parse_args()

    inspired2_train = pickle.load(open('dataset/INSPIRED2/train_pred_aug_dataset_inspired2_final.pkl', 'rb'))
    inspired2_test = pickle.load(open('dataset/INSPIRED2/test_pred_aug_dataset_inspired2_final.pkl', 'rb'))
    gpt_response = json.load(open('response_gen/0904190453_gpt-4.1_inspired2_test_GPT_response_100token.json', 'r', encoding='utf-8'))

    prompt = EVAL_scoring_guide_prompt
    MODEL = "gpt-4.1"

    instructions = []
    for idx, (data, response) in enumerate(zip(inspired2_test, gpt_response)):
        dialog = data['dialog']
        response = response['OUTPUT']
        if not response.strip().startswith('System: '):
            response = 'System: ' + response

        instruction = prompt % (dialog, response)
        instructions.append(instruction)

    mdhm = str(datetime.now(timezone('Asia/Seoul')).strftime('%m%d%H%M%S'))
    if args.log_name == '':
        log_name = f'evaluation/gpt_eval/{mdhm}_{MODEL}_result.json'
    else:
        log_name = args.log_name

    args.log_file = open(os.path.join(log_name), 'a', buffering=1, encoding='UTF-8')

    chatgpt_test(args, instructions=instructions[209:])



    print()
