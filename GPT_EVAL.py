import os
import time
import pickle
import json
import argparse
from datetime import datetime

from openai import OpenAI
from pytz import timezone
from tqdm import tqdm

GPT_EVAL_prompt = """I will provide you with a dialog and a response generated by a CRS.

Dialog:
{dialog}

Response:
{response}

Evaluate the response from each competitor in terms of informativeness, fluency, and relevance on a scale 0 to 5 (0: very bad, 5: very good).

1) Informativeness: Whether the response incorporates rich knowledge
2) Fluency: Whether the response includes diverse words
3) Relevance: Whether the response provides explanations about the recommended item with its features relevant to the dialog context

### Output Format
{"informativeness": <score 0-5>, "fluency": <score 0-5>, "relevance": <score 0-5>}"""

GPT_EVAL_think_prompt = """I will provide you with a dialog and a response generated by a CRS.

Dialog:
%s

Response:
%s

Evaluate the response from each competitor in terms of informativeness, fluency, and relevance on a scale 0 to 5 (0: very bad, 5: very good).

1) Informativeness: Whether the response incorporates rich knowledge
2) Fluency: Whether the response includes diverse words
3) Relevance: Whether the response provides explanations about the recommended item with its features relevant to the dialog context

The reasoning process and answer are enclosed within <think></think> and <answer></answer> tags, respectively, i.e., <think> reasoning process here</think> <answer>answer here</answer>

### Output:
<think>{think}</think>
<answer>{"informativeness": <score 0-5>, "fluency": <score 0-5>, "relevance": <score 0-5>}</answer>"""

GPT_EVAL_think_general_guide_prompt = """I will provide you with a dialog and a response generated by a CRS.

Dialog:
%s

Response:
%s

Evaluate the response from each competitor in terms of informativeness, fluency, and relevance on a scale 0 to 5 (0: very bad, 5: very good).

1) Informativeness: Whether the response incorporates rich knowledge
2) Fluency: Whether the response includes diverse words
3) Relevance: Whether the response provides explanations about the recommended item with its features relevant to the dialog context

Grading rules:
- Be conservative when assigning scores. Do not give 4 or 5 unless the response clearly excels. 
- If a criterion is only partially satisfied, reduce the score (e.g., give 1 or 2 instead of 3). 
- Only responses with strong, explicit evidence of meeting the criterion should receive scores above 3. 
- Use 0 freely for responses that fail to meet the requirement at all.

The reasoning process and answer are enclosed within <think></think> and <answer></answer> tags, respectively, i.e., <think> reasoning process here</think> <answer>answer here</answer>

### Output:
<think>{think}</think>
<answer>{"informativeness": <score 0-5>, "fluency": <score 0-5>, "relevance": <score 0-5>}</answer>"""

EVAL_scoring_guide_prompt = """I will provide you with a dialog and a response generated by a CRS.

Dialog:
%s

Response:
%s

Evaluate the response in terms of informativeness, fluency, and relevance on a scale 1 to 5 (1: very bad, 5: very good).

1) Informativeness: Whether the response incorporates rich knowledge
2) Fluency: Whether the response includes diverse words
3) Relevance: Whether the response recommends proper items and their explanatory features relevant to the dialog context

Guideline for scoring:
- 1 point: The response is very poor. It fails almost entirely to meet this criterion.
- 2 points: The response partially meets this criterion. It shows some adequacy but remains weak or insufficient.
- 3 points: The response meets the minimum requirement for this criterion. It is acceptable but lacks depth, richness, or distinct strengths.
- 4 points: The response performs well on this criterion. It is clear, specific, natural, and contextually appropriate, but not outstanding.
- 5 points: The response fully excels in this criterion. It is rich, highly natural, and strongly aligned with the context. Award only when it clearly stands out.

The reasoning process and answer are enclosed within <think></think> and <answer></answer> tags, respectively, i.e., <think> reasoning process here</think> <answer>answer here</answer>

### Output:
<think>reasoning precess here</think>
<answer>{"informativeness": <score 1-5>, "fluency": <score 1-5>, "relevance": <score 1-5>}</answer>"""

EVAL_scoring_guide_for_target_item_prompt = """I will provide you with a dialog and a response generated by a CRS.

Dialog:
%s

Response:
%s

Evaluate the response in terms of informativeness, fluency, and relevance on a scale from 1 to 5 (1: very bad, 5: very good), under the assumption that the CRS should recommend the target item '%s' in its response.

1) Informativeness: Whether the response incorporates rich knowledge
2) Fluency: Whether the response includes diverse words
3) Relevance: Whether the response recommends the target item appropriately and provides explanatory features relevant to the dialog context

Guideline for scoring:
1 point: The response is very poor. It fails almost entirely to meet this criterion.  
2 points: The response partially meets this criterion. It shows some adequacy but remains weak or insufficient.  
3 points: The response meets the minimum requirement for this criterion. It is acceptable but lacks depth, richness, or distinct strengths.  
4 points: The response performs well on this criterion. It is clear, specific, natural, and contextually appropriate, but not outstanding.  
5 points: The response fully excels in this criterion. It is rich, highly natural, and strongly aligned with the target item and the dialog context. Award only when it clearly stands out.

The reasoning process and answer are enclosed within <think></think> and <answer></answer> tags, respectively, i.e., <think>reasoning process here</think> <answer>answer here</answer>

### Output:
<think>reasoning process here</think>
<answer>{"informativeness": <score 1-5>, "fluency": <score 1-5>, "relevance": <score 1-5>}</answer>"""



EVAL_two_dim = """I will provide you with a dialog and a response generated by a Conversational Recommender System (CRS).

Dialog:
%s

Response:
%s

Evaluate the response along two dimensions: (A) Recommendation Quality and (B) Explanation Quality.

A. Recommendation Quality
1) Validity: Does the recommended item align with the user’s preferences expressed in the dialog?

B. Explanation Quality
1) Informativeness: Does the explanation incorporate rich and meaningful knowledge about the item?
2) Fluency: Is the explanation natural, coherent, and expressed with varied wording?
3) Relevance: Does the explanation highlight item features that are directly relevant to the dialog context?

Scoring: Use a 1–5 scale for each criterion.
- 1 point: Very poor. Fails almost entirely to meet the criterion.
- 2 points: Weak. Shows partial adequacy but remains insufficient.
- 3 points: Moderate. Meets the minimum requirement but lacks depth or strength.
- 4 points: Good. Clear, specific, and contextually appropriate, though not outstanding.
- 5 points: Excellent. Rich, highly natural, and strongly aligned with the context. Award only if it clearly stands out.

Output format:
<think>reasoning process here</think>
<answer>{"validity": <1–5>, "informativeness": <1–5>, "fluency": <1–5>, "relevance": <1–5>}</answer>"""


EVAL_only_response = """I will provide you with a dialog and a response generated by a Conversational Recommender System (CRS).

Dialog:
%s

Response:
%s

Evaluate the response along explanation quality.
1) Informativeness: Does the explanation incorporate rich and meaningful knowledge about the recommended item?
2) Fluency: Is the explanation natural, coherent, and expressed with varied wording?
3) Relevance: Does the explanation highlight the features of the recommended item that are directly relevant to the dialog context?

Scoring: Use a 1–5 scale for each criterion.
- 1 point: Very poor. Fails almost entirely to meet the criterion.
- 2 points: Weak. Shows partial adequacy but remains insufficient.
- 3 points: Moderate. Meets the minimum requirement but lacks depth or strength.
- 4 points: Good. Clear, specific, and contextually appropriate, though not outstanding.
- 5 points: Excellent. Rich, highly natural, and strongly aligned with the context. Award only if it clearly stands out.

Output format:
<think>reasoning process here</think>
<answer>{"informativeness": <1–5>, "fluency": <1–5>, "relevance": <1–5>}</answer>"""


Normal_Evaluator = """I will provide you with a dialog and a response generated by a Conversational Recommender System (CRS).

Dialog:
%s

Response:
%s

Evaluate the response along explanation quality.
1) Informativeness: Does the explanation incorporate rich and meaningful knowledge about the recommended item?
2) Fluency: Is the explanation natural, coherent, and expressed with varied wording?
3) Relevance: Does the explanation highlight the features of the recommended item that are directly relevant to the dialog context?

Scoring: Use a 1–5 scale for each criterion.
- 1 point: Very poor. Fails almost entirely to meet the criterion.
- 2 points: Weak. Shows partial adequacy but remains insufficient.
- 3 points: Moderate. Meets the minimum requirement but lacks depth or strength.
- 4 points: Good. Clear, specific, and contextually appropriate, though not outstanding.
- 5 points: Excellent. Rich, highly natural, and strongly aligned with the context. Award only if it clearly stands out.

Output format:
<think>reasoning process here</think>
<answer>{"informativeness": <1–5>, "fluency": <1–5>, "relevance": <1–5>}</answer>"""


Strict_Evaluator = '''I will provide you with a dialog and a response generated by a Conversational Recommender System (CRS).

Dialog:
%s

Response:
%s

Evaluate the response along explanation quality.
1) Informativeness: Does the explanation incorporate rich and meaningful knowledge about the recommended item?
2) Fluency: Is the explanation natural, coherent, and expressed with varied wording?
3) Relevance: Does the explanation highlight the features of the recommended item that are directly relevant to the dialog context?

Scoring: Use a 1–5 scale for each criterion.
- 1 point: Very poor. Fails almost entirely to meet the criterion.
- 2 points: Weak. Shows partial adequacy but remains insufficient.
- 3 points: Moderate. Meets the minimum requirement but lacks depth or strength.
- 4 points: Good. Clear, specific, and contextually appropriate, though not outstanding.
- 5 points: Excellent. Rich, highly natural, and strongly aligned with the context. Award only if it clearly stands out.

Act as a strict evaluator: unless the response is virtually flawless for a criterion, cap the score at 3. 
Do not award 4 or 5 unless it is near-perfect for that criterion.

Output format:
<think>reasoning process here</think>
<answer>{"informativeness": <1–5>, "fluency": <1–5>, "relevance": <1–5>}</answer>'''


def parse_args():
    parser = argparse.ArgumentParser()
    parser.add_argument('--access_token', type=str, default="")
    parser.add_argument('--gpt_model', type=str, default="")

    parser.add_argument('--cnt', type=int, default=0)
    parser.add_argument('--end', type=int, default=0)

    parser.add_argument('--log_name', type=str, default="")

    parser.add_argument('--dataset_path', type=str, default="")
    parser.add_argument('--response_path', type=str, default="")
    parser.add_argument('--only_6_turn', action='store_true')

    parser.add_argument('--prompt_mode', type=str, default="")
    args = parser.parse_args()
    return args


def execute(args,
            instructions: list = None,
            labels: list = None,
            inputs: list = None):
    # openai.api_key = args.access_token
    client = OpenAI(api_key=args.access_token)

    cnt = args.cnt
    for instruction in tqdm(instructions[cnt:], bar_format=' {percentage:3.0f} % | {bar:23} {r_bar}'):

        # content = template % (instruction)
        # content = mentioned_reviews_concat + refinement_template % instruction
        # print()
        try:
            response = client.chat.completions.create(
                model=MODEL,
                messages=[
                    {"role": "user",
                     # "content": review_summary_template % (label, instruction, label)}
                     "content": instruction}
                ],
                temperature=0,
            )

            response = response.choices[0].message.content

            args.log_file.write(
                json.dumps({'INPUT': instruction, 'OUTPUT': response}, ensure_ascii=False, indent=4) + '\n')
            cnt += 1

        except Exception as error:
            print("ERROR cnt: %d" % (cnt))
            print(args.log_file)
            args.cnt = int(cnt)
            time.sleep(5)
            break

        # openai.api_requestor._thread_context.session.close()
        if int(cnt) == len(instructions):
            return False


def chatgpt_test(args,
                 instructions: list = None,
                 labels: list = None,
                 inputs: list = None,
                 ):
    print('CHATGPT_TEST_START')
    while True:
        if execute(args=args, instructions=instructions, labels=labels, inputs=inputs) == False:
            break


if __name__ == "__main__":

    args = parse_args()

    if 'pkl' in args.dataset_path:
        dataset = pickle.load(open(args.dataset_path, 'rb'))
    elif 'json' in args.dataset_path:
        dataset = json.load(open(args.dataset_path, 'r', encoding='utf-8'))
    else:
        print('Dataset not found.')
    gpt_response = json.load(open(args.response_path, 'r', encoding='utf-8'))

    prompt = Normal_Evaluator if args.prompt_mode == "normal" else Strict_Evaluator
    MODEL = args.gpt_model #"gpt-4.1-mini"
    print(f'GPT MODEL: {MODEL}')

    response_key = list(gpt_response[0].keys())[-1]

    instructions = []
    # if args.response_path == "":
    #     key = 'response'
    # else:
    #     key = 'OUTPUT'
    for idx, (data, response) in enumerate(zip(dataset, gpt_response)):
        dialog = data['dialog']
        if args.only_6_turn:
            dialogs = dialog.split('\n')[-6:]
            dialog = ''
            for d in dialogs:
                dialog += d
                dialog += '\n'
            dialog = dialog.strip()


        response = response[response_key].split('<item>')[-1].split('</item')[-1].split('<answer>')[-1].split('</answer>')[0].strip()
        if not response.strip().startswith('System: '):
            response = 'System: ' + response
        # target_item = data['topic']

        instruction = prompt % (dialog, response)
        instructions.append(instruction)

    mdhm = str(datetime.now(timezone('Asia/Seoul')).strftime('%m%d%H%M%S'))
    if args.log_name == '':
        log_name = f'evaluation/gpt_eval/{mdhm}_{MODEL}_result.json'
    else:
        log_name = f'evaluation/gpt_eval/{mdhm}_{MODEL}_eval_{args.log_name}'

    args.log_file = open(os.path.join(log_name), 'a', buffering=1, encoding='UTF-8')

    chatgpt_test(args, instructions=instructions)

    print()
