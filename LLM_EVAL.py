import os
import time
import pickle
import json
import argparse
from datetime import datetime

from litellm.proxy.proxy_server import folder_name
from openai import OpenAI
from google import genai

from pytz import timezone
from tqdm import tqdm


LLM_judge_prompt = """I will provide you with a dialog and a response generated by a Conversational Recommender System (CRS).

Dialog:
%s

Response:
%s

Evaluate the response along explanation quality.
1) Informativeness: Does the explanation incorporate rich and meaningful knowledge about the recommended item?
2) Fluency: Is the explanation natural, coherent, and expressed with varied wording?
3) Relevance: Does the explanation highlight the features of the recommended item that are directly relevant to the dialog context?

Scoring: Use a 1–5 scale for each criterion.
- 1 point: Very poor. Fails almost entirely to meet the criterion.
- 2 points: Weak. Shows partial adequacy but remains insufficient.
- 3 points: Moderate. Meets the minimum requirement but lacks depth or strength.
- 4 points: Good. Clear, specific, and contextually appropriate, though not outstanding.
- 5 points: Excellent. Rich, highly natural, and strongly aligned with the context. Award only if it clearly stands out.

Output format:
<think>reasoning process here</think>
<answer>{"informativeness": <1–5>, "fluency": <1–5>, "relevance": <1–5>}</answer>"""


def parse_args():
    parser = argparse.ArgumentParser()
    parser.add_argument('--access_token', type=str, default="")
    parser.add_argument('--LLM_model', type=str, default="")

    parser.add_argument('--cnt', type=int, default=0)
    parser.add_argument('--end', type=int, default=0)

    parser.add_argument('--log_name', type=str, default="")

    parser.add_argument('--dataset_path', type=str, default="")
    parser.add_argument('--response_path', type=str, default="")
    parser.add_argument('--only_6_turn', action='store_true')

    args = parser.parse_args()
    return args


def execute(args,
            instructions: list = None,
            labels: list = None,
            inputs: list = None):
    # openai.api_key = args.access_token
    client = OpenAI(api_key=args.access_token)

    cnt = args.cnt
    for instruction in tqdm(instructions[cnt:], bar_format=' {percentage:3.0f} % | {bar:23} {r_bar}'):

        # content = template % (instruction)
        # content = mentioned_reviews_concat + refinement_template % instruction
        # print()
        try:
            response = client.chat.completions.create(
                model=MODEL,
                messages=[
                    {"role": "user",
                     # "content": review_summary_template % (label, instruction, label)}
                     "content": instruction}
                ],
                temperature=0,
            )

            response = response.choices[0].message.content

            args.log_file.write(
                json.dumps({'INPUT': instruction, 'OUTPUT': response}, ensure_ascii=False, indent=4) + '\n')
            cnt += 1

        except Exception as error:
            print("ERROR cnt: %d" % (cnt))
            print(args.log_file)
            args.cnt = int(cnt)
            time.sleep(5)
            break

        # openai.api_requestor._thread_context.session.close()
        if int(cnt) == len(instructions):
            return False


def chatgpt_test(args,
                 instructions: list = None,
                 labels: list = None,
                 inputs: list = None,
                 ):
    print('CHATGPT_TEST_START')
    while True:
        if execute(args=args, instructions=instructions, labels=labels, inputs=inputs) == False:
            break


def gemini_gest(args,
                instructions: list = None,
                labels: list = None,
                inputs: list = None
                ):

    print('GEMINI START')
    cnt = args.cnt
    client = genai.Client(api_key=args.access_token)

    pbar = tqdm(total=len(instructions), initial=cnt, bar_format=' {percentage:3.0f} % | {bar:30} {n_fmt}/{total_fmt}')
    while cnt < len(instructions):
        instruction = instructions[cnt]
        try:
            response = client.models.generate_content(model=args.gemini_model, contents=instruction)
            response = response.text

            args.log_file.write(
                json.dumps({'INPUT': instruction, 'OUTPUT': response}, ensure_ascii=False, indent=4) + '\n')
            cnt += 1
            pbar.update(1)

        except Exception as error:
            print("%s | ERROR cnt: %d" % (error, cnt))
            print(args.log_file)
            time.sleep(5)


if __name__ == "__main__":

    args = parse_args()

    if 'pkl' in args.dataset_path:
        dataset = pickle.load(open(args.dataset_path, 'rb'))
    elif 'json' in args.dataset_path:
        dataset = json.load(open(args.dataset_path, 'r', encoding='utf-8'))
    else:
        print('Dataset not found.')
    gpt_response = json.load(open(args.response_path, 'r', encoding='utf-8'))

    prompt = LLM_judge_prompt
    MODEL = args.LLM_model #"gpt-4.1-mini"
    print(f'LLM JUDGE: {MODEL}')

    response_key = list(gpt_response[0].keys())[-1]

    instructions = []
    for idx, (data, response) in enumerate(zip(dataset, gpt_response)):
        dialog = data['dialog']
        if args.only_6_turn:
            dialogs = dialog.split('\n')[-6:]
            dialog = ''
            for d in dialogs:
                dialog += d
                dialog += '\n'
            dialog = dialog.strip()


        response = response[response_key].split('<item>')[-1].split('</item')[-1].split('<answer>')[-1].split('</answer>')[0].strip()
        if not response.strip().startswith('System: '):
            response = 'System: ' + response
        # target_item = data['topic']

        instruction = prompt % (dialog, response)
        instructions.append(instruction)

    mdhm = str(datetime.now(timezone('Asia/Seoul')).strftime('%m%d%H%M%S'))
    if 'gpt' in args.LLM_model:
        folder_name = 'gpt_eval'
    elif 'gemini' in args.LLM_model:
        folder_name = 'gemini_eval'

    if args.log_name == '':
        log_name = f'evaluation/{folder_name}/{mdhm}_{MODEL}_eval.json'
    else:
        log_name = f'evaluation/{folder_name}/{mdhm}_{MODEL}_eval_{args.log_name}.json'

    args.log_file = open(os.path.join(log_name), 'a', buffering=1, encoding='UTF-8')

    if 'gpt' in args.LLM_model:
        chatgpt_test(args, instructions=instructions)
    elif 'gemini' in args.LLM_model:
        gemini_gest(args, instructions=instructions)

    print()
